{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9409f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aleksejkitajskij/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import yaml\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import spacy\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors #  implements word vectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1870ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Module_5_Lecture_1_Class_amazon_product_reviews.csv', index_col='Id')\n",
    "df['sentiment'] = [1 if score in [4, 5] else 0 for score in df['Score']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df = df.drop_duplicates(subset={\"UserId\", \"Time\",\"Text\"})\n",
    "df = df.groupby('sentiment').sample(2500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7b7f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ProductId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "UserId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ProfileName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "HelpfulnessNumerator",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "HelpfulnessDenominator",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Time",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b59c62fb-b598-479b-92c6-9afed9d1e845",
       "rows": [
        [
         "556850",
         "B001EO5YDY",
         "A3KMNPL0AN0QB3",
         "Chad Ware",
         "1",
         "1",
         "3",
         "1326240000",
         "Flavors in product description are inaccurate",
         "According to the product description, I was supposed to received 2 sampler boxes that contained 22 k-cups with the following flavors: Hazelnut Cream, French Vanilla, and Rain Forest Nut.<br /><br />The boxes that I received had French Vanilla, Caramel Vanilla Cream, Hazelnut, and Wild Mountain Blueberry. I was disappointed because I really wanted the Rain Forest Nut (but didn't get it) and I really don't like the blueberry flavor (already had a box of it).<br /><br />The coffee is good, but don't be surprised if you get different flavors than what you were expecting.",
         "0"
        ],
        [
         "36204",
         "B000FGXT2A",
         "A35DW1GJBLNMZI",
         "VicPaxGear",
         "1",
         "1",
         "3",
         "1202860800",
         "Tasty but Crumbly",
         "This cereal tastes great.  Unfortunately, it doesn't stand up to the shipping very well.  A disproportionate amount of every box I ordered was just shredded crumbs.  I've bought this cereal in the store and this was not an issue.  It could just be my careless delivery people, but I stopped this subscription.",
         "0"
        ],
        [
         "506099",
         "B001C15JCU",
         "A26LT2ZMC3E0BK",
         "C. Fairstone",
         "10",
         "11",
         "1",
         "1346198400",
         "FDA Warns Chicken Jerky From China May Harm or Kill Your Dog.",
         "STOP USING THIS PRICEY JUNK NOW!! search \"chicken jerky treats china\" on google for more info and SPREAD THE WORD (some media coverage but way too little...) I am \"fortunate\"... my dog only suffered small amount of kidney damage-- others are not so lucky. Remember the Chinese pet food scandal in 2007 that killed 1000's of our best friends. TELL AMAZON  NOT TO SELL CHINESE CHICKEN JERKY until it is proven saFe (and even then I'd avoid it)",
         "0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>556850</th>\n",
       "      <td>B001EO5YDY</td>\n",
       "      <td>A3KMNPL0AN0QB3</td>\n",
       "      <td>Chad Ware</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1326240000</td>\n",
       "      <td>Flavors in product description are inaccurate</td>\n",
       "      <td>According to the product description, I was su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36204</th>\n",
       "      <td>B000FGXT2A</td>\n",
       "      <td>A35DW1GJBLNMZI</td>\n",
       "      <td>VicPaxGear</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1202860800</td>\n",
       "      <td>Tasty but Crumbly</td>\n",
       "      <td>This cereal tastes great.  Unfortunately, it d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506099</th>\n",
       "      <td>B001C15JCU</td>\n",
       "      <td>A26LT2ZMC3E0BK</td>\n",
       "      <td>C. Fairstone</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1346198400</td>\n",
       "      <td>FDA Warns Chicken Jerky From China May Harm or...</td>\n",
       "      <td>STOP USING THIS PRICEY JUNK NOW!! search \"chic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ProductId          UserId   ProfileName  HelpfulnessNumerator  \\\n",
       "556850  B001EO5YDY  A3KMNPL0AN0QB3     Chad Ware                     1   \n",
       "36204   B000FGXT2A  A35DW1GJBLNMZI    VicPaxGear                     1   \n",
       "506099  B001C15JCU  A26LT2ZMC3E0BK  C. Fairstone                    10   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "556850                       1      3  1326240000   \n",
       "36204                        1      3  1202860800   \n",
       "506099                      11      1  1346198400   \n",
       "\n",
       "                                                  Summary  \\\n",
       "556850      Flavors in product description are inaccurate   \n",
       "36204                                   Tasty but Crumbly   \n",
       "506099  FDA Warns Chicken Jerky From China May Harm or...   \n",
       "\n",
       "                                                     Text  sentiment  \n",
       "556850  According to the product description, I was su...          0  \n",
       "36204   This cereal tastes great.  Unfortunately, it d...          0  \n",
       "506099  STOP USING THIS PRICEY JUNK NOW!! search \"chic...          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81151f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/my_vocab.yaml', 'r') as file:\n",
    "    vocab = yaml.safe_load(file)\n",
    "\n",
    "contractions = vocab['contractions']\n",
    "negations = vocab['negations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c94b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negations ['aren', \"aren't\", 'couldn', \"couldn't\", 'didn']\n",
      "contractions [(\"ain't\", 'am not'), (\"aren't\", 'are not'), (\"can't\", 'cannot'), (\"can't've\", 'cannot have'), ('cause', 'because')]\n"
     ]
    }
   ],
   "source": [
    "print('negations', negations[:5])\n",
    "print('contractions', [(k, contractions[k]) for k in list(contractions.keys())[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ef4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# including words to stop-words list\n",
    "include_to_stopwords = set(['also', 'would', 'much', 'many'])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = stop_words.union(include_to_stopwords)\n",
    "\n",
    "# removing words from the stop-words list\n",
    "stop_words = stop_words.difference(negations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78aa1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['parser','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf04ae52",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def remove_stuff(text):\n",
    "    text = re.sub(\"<[^>]*>\", \" \", text) # Remove html tags\n",
    "    text = re.sub(\"\\S*@\\S*[\\s]+\", \" \", text) # Remove emails\n",
    "    text = re.sub(\"https?:\\/\\/.*?[\\s]+\", \" \", text) # Remove links\n",
    "    text = re.sub(\"[^a-zA-Z' ]\", \"\", text) # Remove non-letters\n",
    "    text = re.sub(\"[\\s]+\", \" \", text) # Remove excesive whitespaces\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stop_words: set):\n",
    "    text = text.lower().split()\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def process_with_stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = text.lower().split()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def process_with_lemmatizer(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc if len(token.lemma_) > 1 ])\n",
    "\n",
    "    return text\n",
    "\n",
    "def replace_words(text, replace_on:dict):\n",
    "    text = text.lower().split()\n",
    "    text = [replace_on.get(word) if word in replace_on else word for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = remove_stuff(text)\n",
    "    text = remove_stopwords(text, stop_words)\n",
    "    text = replace_words(text, contractions)\n",
    "\n",
    "    # test = process_with_stemmer(text)\n",
    "    text = process_with_lemmatizer(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d59189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "On a quest for the perfedc1112t,,, !!!! <br />%%2%% popcorn to compliment the Whirley Pop.  Don't get older, I'm beginning to appreciate the more \"natural\" popcorn varieties, and I suppose that's what attracted me to the Arrowhead Mills Organic Yellow Popcorn.<br /> <br />I'm no \"organic\" food expert.  I just wanted some good tasting popcorn.  And, I feel like that's what I got.  Using the Whirley Pop, with a very small amount of oil, I've had great results.\n",
      "##################################################\n",
      "Normalized text\n",
      "quest perfedct popcorn compliment whirley pop do not get old begin appreciate natural popcorn variety suppose that attract arrowhead mill organic yellow popcorn organic food expert want good tasting popcorn feel like that get use whirley pop small amount oil ve great result\n"
     ]
    }
   ],
   "source": [
    "text = 'On a quest for the perfedc1112t,,, !!!! <br />%%2%% popcorn to compliment\\\n",
    " the Whirley Pop.  Don\\'t get older, I\\'m beginning to appreciate the more \"natural\" \\\n",
    "popcorn varieties, and I suppose that\\'s what attracted me to the Arrowhead Mills \\\n",
    "Organic Yellow Popcorn.<br /> <br />I\\'m no \"organic\" food expert.  I just wanted \\\n",
    "some good tasting popcorn.  And, I feel like that\\'s what I got.  Using the Whirley \\\n",
    "Pop, with a very small amount of oil, I\\'ve had great results.'\n",
    "\n",
    "print('Original text')\n",
    "print(text)\n",
    "print(\"#\" * 50)\n",
    "print('Normalized text')\n",
    "print(normalize_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d249ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cfeeb5ddf341cf8924a12f54d401da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text_normalized'] = df['Text'].progress_apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0831aa",
   "metadata": {},
   "source": [
    "### scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3db67fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accord',\n",
       " 'product',\n",
       " 'description',\n",
       " 'suppose',\n",
       " 'receive',\n",
       " 'sampler',\n",
       " 'box',\n",
       " 'contain',\n",
       " 'kcup',\n",
       " 'follow']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for sentence in data:\n",
    "        word_list = sentence.split(\" \")\n",
    "        corpus.append(word_list)    \n",
    "           \n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(df['text_normalized'])\n",
    "corpus[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb_from_scratch = word2vec.Word2Vec(corpus, vector_size=100, window=5, min_count=50, workers=4)\n",
    "model_emb_from_scratch.wv.save_word2vec_format('data/model_emb_from_scratch.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42969ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok2vec(tokens, model, avg = 'mean'):\n",
    "    vects = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vects.append(model[token])\n",
    "        except TypeError:\n",
    "            try:\n",
    "                vects.append(model.wv[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    if not vects:\n",
    "        return np.full((model.vector_size,), np.nan, dtype=np.float32)\n",
    "\n",
    "    vects = np.array(vects)\n",
    "\n",
    "    if avg == 'mean':\n",
    "        return np.nanmean(vects, axis=0)\n",
    "    elif avg == 'sum':\n",
    "        return np.nansum(vects, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae920e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['this', 'is', 'an', 'example', 'cat', '.']\n",
      "vector [ 0.59161085  0.35044494  0.8492645   0.46546075 -0.25025776 -0.6378975\n",
      "  0.12813044  0.36764082  0.4150986   0.42065218 -0.17782676 -0.20056008\n",
      "  0.6615182   0.8868185   0.14721219 -0.01239322 -0.3462136  -0.22809528\n",
      "  0.14166239 -0.90407175  0.30769414  1.1139303  -0.5064335  -0.3028266\n",
      " -1.2784158   0.99183035 -0.45414108 -0.5351317  -0.24835297  0.4142786\n",
      "  0.37300763 -0.00327196  0.60698575 -0.09981637 -0.57618225  0.23192622\n",
      " -0.31584084 -0.07097046 -0.47449717 -1.0083066   0.29220402 -0.4681635\n",
      " -0.8912367  -0.12310207 -0.25458217 -0.07787346 -0.09584736 -0.36006615\n",
      "  0.43161273 -0.5658284  -0.32748434  0.32925162 -0.11703091  0.31460857\n",
      "  0.40999267  0.88384813 -0.27076566 -0.39030018  0.48422053  0.11924732\n",
      "  0.20880312 -0.7001594  -0.43048787 -0.3184659  -0.38061136  0.42503923\n",
      "  0.5088034  -0.2238752  -0.45792094  0.30862778 -0.65808004 -0.15626599\n",
      " -0.6572367  -0.69766015 -0.12925617  1.4608196  -0.5171097   0.4442047\n",
      " -0.45323077 -0.24981047 -0.57846934  0.7929122  -0.14608675  1.1778643\n",
      " -1.0894556   0.01050784  0.31322756  0.5611284   1.1871022  -0.1610921\n",
      "  0.02199489 -0.35366568 -0.6451616  -0.04693858  0.85628545  0.08955735\n",
      " -0.3585309  -0.5560601  -0.05483987 -0.14347059]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example cat.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "print('tokens', tokens)\n",
    "\n",
    "vector = tok2vec(tokens, model_emb_from_scratch, avg='mean')\n",
    "print('vector', vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d14d6",
   "metadata": {},
   "source": [
    "### glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2606ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/q4zhf1wx08z6l2f4bqz5b8dc0000gn/T/ipykernel_49289/2222633575.py:2: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec('data/glove.6B.50d.txt', 'data/glove.6B.50d.vec')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat': [ 0.45281  -0.50108  -0.53714  -0.015697  0.22191   0.54602  -0.67301\n",
      " -0.6891    0.63493  -0.19726   0.33685   0.7735    0.90094   0.38488\n",
      "  0.38367   0.2657   -0.08057   0.61089  -1.2894   -0.22313  -0.61578\n",
      "  0.21697   0.35614   0.44499   0.60885  -1.1633   -1.1579    0.36118\n",
      "  0.10466  -0.78325   1.4352    0.18629  -0.26112   0.83275  -0.23123\n",
      "  0.32481   0.14485  -0.44552   0.33497  -0.95946  -0.097479  0.48138\n",
      " -0.43352   0.69455   0.91043  -0.28173   0.41637  -1.2609    0.71278\n",
      "  0.23782 ]\n",
      "Most similar words to 'cat': [('dog', 0.9218006134033203), ('rabbit', 0.8487821817398071), ('monkey', 0.8041081428527832), ('rat', 0.7891963720321655), ('cats', 0.7865270376205444)]\n"
     ]
    }
   ],
   "source": [
    "# Convert GloVe format to Word2Vec format\n",
    "glove2word2vec('data/glove.6B.50d.txt', 'data/glove.6B.50d.vec')\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = KeyedVectors.load_word2vec_format('data/glove.6B.50d.vec')\n",
    "\n",
    "# Example: Retrieve vector for a word\n",
    "word_vector = glove_model['cat']\n",
    "print(\"Vector for 'cat':\", word_vector)\n",
    "\n",
    "# Example: Find most similar words\n",
    "similar_words = glove_model.most_similar('cat', topn=5)\n",
    "print(\"Most similar words to 'cat':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b2e38",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad8aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_normalized']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163a8eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one big mouser love troutyet another high quality cat food fancy feast appreciate amazon offer variety autodelivery discount think cat like troutit not smell stinky seem type gravy kitty nice bath eat appearing satifie what not like happy cat happy human\n"
     ]
    }
   ],
   "source": [
    "print(X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d0d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'big', 'mouser', 'love', 'troutyet', 'another', 'high', 'quality', 'cat', 'food', 'fancy', 'feast', 'appreciate', 'amazon', 'offer', 'variety', 'autodelivery', 'discount', 'think', 'cat', 'like', 'troutit', 'not', 'smell', 'stinky', 'seem', 'type', 'gravy', 'kitty', 'nice', 'bath', 'eat', 'appearing', 'satifie', 'what', 'not', 'like', 'happy', 'cat', 'happy', 'human']\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.apply(word_tokenize)\n",
    "print(X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0758bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23401967  0.11642583 -0.39039063 -0.19259414  0.44021174  0.08487431\n",
      " -0.44889867 -0.25670952  0.08128236  0.2058957  -0.04263912  0.38225102\n",
      "  0.18541351  0.02980959  0.47350112  0.2496316   0.12010796  0.1731719\n",
      " -0.1827333  -0.62209094 -0.13758871  0.17337418  0.26171416  0.22553168\n",
      "  0.24107078 -1.0848439  -0.7436808   0.31928018  0.57311535 -0.4045977\n",
      "  2.3049736   0.39121112 -0.0859626   0.0529886   0.04741706  0.05078922\n",
      " -0.08203696  0.08052175  0.11566141 -0.36979553 -0.04784045  0.19128942\n",
      " -0.02155017  0.31916544  0.3836128   0.17888433 -0.02071665 -0.21769795\n",
      "  0.09551313  0.33448845]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.apply(lambda x: tok2vec(x, glove_model, 'mean'))\n",
    "print(X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1cf13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23401967  0.11642583 -0.39039063 -0.19259414  0.44021174  0.08487431\n",
      " -0.44889867 -0.25670952  0.08128236  0.2058957  -0.04263912  0.38225102\n",
      "  0.18541351  0.02980959  0.47350112  0.2496316   0.12010796  0.1731719\n",
      " -0.1827333  -0.62209094 -0.13758871  0.17337418  0.26171416  0.22553168\n",
      "  0.24107078 -1.0848439  -0.7436808   0.31928018  0.57311535 -0.4045977\n",
      "  2.3049736   0.39121112 -0.0859626   0.0529886   0.04741706  0.05078922\n",
      " -0.08203696  0.08052175  0.11566141 -0.36979553 -0.04784045  0.19128942\n",
      " -0.02155017  0.31916544  0.3836128   0.17888433 -0.02071665 -0.21769795\n",
      "  0.09551313  0.33448845]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f9d95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.apply(word_tokenize)\n",
    "X_test = X_test.apply(lambda x: tok2vec(x, glove_model, 'mean'))\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2ff6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "X_train = np.stack(X_train, axis=0)\n",
    "X_test = np.stack(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0cd7314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 50), (1000, 50), (4000,), (1000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb152889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.727\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
